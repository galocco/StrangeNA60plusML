NBODY: 2
FILE_PREFIX: 'PHI_L5_E40_analysis'

PDG: 333
EINT: 40
MULTIPLICITY: 2.55
BRATIO: 0.489
T: 0.244
EFF: 0.191 #0.528
SIGMA: 0.0025

CENTRALITY_CLASS: 
  - [0, 5]
CT_BINS: [0, 100]
PT_BINS: [0, 3.]
BKG_MODELS: ['expo', 'pol1', 'pol2']

BDT_EFFICIENCY: [0.01, 0.99, 0.01] ##min, max ,step

MC_PATH: ../Data/PHI_L5_E40/fntSig_PHI_L5_E40_TS.root
BKG_PATH: ../Data/PHI_L5_E40/fntBkg_PHI_L5_E40_TS.root
MC_PATH_FULL: ../Data/PHI_L5_E40/fntBkg_PHI_Full_L5_E40_TS.root
DATA_SIG_PATH: ['../Data/PHI_L5_E40/fntSig_PHI_L5_E40_Data.root','../Data/PHI_L5_E40/fntSig_PHI_L5_E40_Data.root','../Data/PHI_L5_E40/fntSig_PHI_L5_E40_Data.root','../Data/PHI_L5_E40/fntSig_PHI_L5_E40_Data.root','../Data/PHI_L5_E40/fntSig_PHI_L5_E40_Data.root']
DATA_BKG_PATH: ['../Data/PHI_L5_E40/fntBkg_PHI_L5_E40_Data_0.root','../Data/PHI_L5_E40/fntBkg_PHI_L5_E40_Data_1.root','../Data/PHI_L5_E40/fntBkg_PHI_L5_E40_Data_2.root','../Data/PHI_L5_E40/fntBkg_PHI_L5_E40_Data_3.root','../Data/PHI_L5_E40/fntBkg_PHI_L5_E40_Data_4.root']
DATA_PATH: ['../Data/PHI_L5_E40/fntBkg_PHI_Full_L5_E40_Data.root']
EVENT_PATH: ['../Data/PHI_L5_E40/Bkg-histos_PHI_L5_E40_Data_0.root','../Data/PHI_L5_E40/Bkg-histos_PHI_L5_E40_Data_1.root','../Data/PHI_L5_E40/Bkg-histos_PHI_L5_E40_Data_2.root','../Data/PHI_L5_E40/Bkg-histos_PHI_L5_E40_Data_3.root','../Data/PHI_L5_E40/Bkg-histos_PHI_L5_E40_Data_4.root']
EVENT_PATH_FULL: ['../Data/PHI_L5_E40/Bkg-histos_PHI_Full_L5_E40_Data.root']

PRESELECTION: 'pt > 0.9 and thetad > 0.005 and thetad < 0.15' #and rapidity > 2.9 and rapidity < 3.7 and thetad < 0.999999996 and thetad > 0.999996573' #dist<1 and d0prod < 0.1 and d0prod > -0.1 and dca< 0.018 and thetad > 0.999 and arm < 0.8 and arm > -0.8

LARGE_DATA: True
GAUSS: False

XGBOOST_PARAMS:
  # general parameters
  silent: 1 # print message (useful to understand whats happening)
  n_jobs: 8 # number of available threads
  # learning task parameters
  objective: binary:logistic
  random_state: 42
  eval_metric: auc
  tree_method: hist

HYPERPARAMS:
  max_depth: 7
  learning_rate: 0.167
  n_estimators: 83
  gamma: 0.525
  min_child_weight: 9.82
  subsample: 0.89
  colsample_bytree: 0.64
  seed : 42

HYPERPARAMS_RANGE: #TODO: check if it works without tuples
  # booster parameters
  max_depth: !!python/tuple [5, 20] # defines the maximum depth of a single tree (regularization)
  learning_rate: !!python/tuple [0.01, 0.3] # learning rate
  n_estimators: !!python/tuple [50, 500] # number of boosting trees
  gamma: !!python/tuple [0.3, 1.1] # specifies the minimum loss reduction required to make a split
  min_child_weight: !!python/tuple [1, 12]
  subsample: !!python/tuple [0.5, 0.9] # denotes the fraction of observations to be randomly samples for each tree
  colsample_bytree: !!python/tuple [0.5, 0.9] # denotes the fraction of columns to be randomly samples for each tree
  # # lambda: (0,10]  # L2 regularization term on weights
  # # alpha: (0,10]  # L1 regularization term on weight

TRAINING_COLUMNS: 
  - cosp
  #- ct
  - dist
  - dca
  #- rapidity
  - d0prod
  #- d01
  #- d02
  - ptMin
  #- arm
  #- ptMax
  #- ipD